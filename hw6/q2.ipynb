{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 6 Q2\n",
        "In this question, we will be analyzing blood cell images! There will be 3 classes: basophil, eosinophil, and neutrophil. Your job is to write convolutional neural networks to classify these images as accurately as you can."
      ],
      "metadata": {
        "id": "cd-mIglIgZ9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Preprocess Image Data"
      ],
      "metadata": {
        "id": "IdPkg4wPdwlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyqnSbN5XMRZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "import random"
      ],
      "metadata": {
        "id": "CcFaLhA4XP3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly write down our class indices for consistency throughout the program!"
      ],
      "metadata": {
        "id": "R4IWWZr6eCZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"basophil\", \"eosinophil\", \"neutrophil\"]\n",
        "class2idx = {\"basophil\" : 0, \"eosinophil\" : 1, \"neutrophil\" : 2}\n",
        "idx2class = {0 : \"basophil\", 1 : \"eosinophil\", 2 : \"neutrophil\"}"
      ],
      "metadata": {
        "id": "NmMukQ_-eGeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will gather all the file names now and shuffle them..."
      ],
      "metadata": {
        "id": "pVom_AKo5t8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/YOUR/PATH/TO/bloodcells_dataset/\"     # make sure it ends with bloodcells_dataset/ so we can access the different subfolders\n",
        "baso = [f for f in listdir(file_path + \"basophil\") if isfile(join(file_path + \"basophil\", f)) and f != \".DS_Store\"]\n",
        "eosi = [f for f in listdir(file_path + \"eosinophil\") if isfile(join(file_path + \"eosinophil\", f)) and f != \".DS_Store\"]\n",
        "neutro = [f for f in listdir(file_path + \"neutrophil\") if isfile(join(file_path + \"neutrophil\", f)) and f != \".DS_Store\"]\n",
        "data = baso + eosi + neutro\n",
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "DsBtTrQDdGHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build a class called `BloodCell_Dataset`, a subclass of PyTorch's `torch.utils.data.Dataset`. All subclasses need to define 3 methods: `__init__()`, `__len__()`, and `__getitem__()`.\n",
        "\n",
        "\n",
        "*   `__init__()`: Takes in the file path of the image folders, the list of file names we just gathered, and one of two modes - `train` or `test`. If the mode is `train`, then we will create the training test. If the mode is `test`, the entire testing data will be fetched at once to save time as we don't have that many testing images.\n",
        "*   `__len__()`: Returns the length of the dataset, # of samples.\n",
        "*   `__getitem__()`: Gets an input `i` and returns a tuple of `(data[i], label[i])`. In this method, we will only read the image from the disk every time a sample is requested. This helps us save an enourmous amount of space and can ensure CUDA out of memory errors never happening. This method also does some quick preprocessing, it makes sure that all images are of size `(3, 363, 360)`.\n",
        "\n",
        "In the future when you handle large datasets, you will very likely need to handle it with this fashion. If we were to fetch ALL our images from disk, we will likely run out of memory very quickly because we are dealing with high resolution images.\n",
        "\n"
      ],
      "metadata": {
        "id": "bVra5vz55zv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BloodCell_Dataset(Dataset):\n",
        "    def __init__(self, file_path, file_path_list, mode=\"train\", test_size=0.2):\n",
        "        self.file_path = file_path\n",
        "        self.classes = [\"basophil\", \"eosinophil\", \"neutrophil\"]\n",
        "        self.class2idx = {\"basophil\" : 0, \"eosinophil\" : 1, \"neutrophil\" : 2}\n",
        "        self.idx2class = {0 : \"basophil\", 1 : \"eosinophil\", 2 : \"neutrophil\"}\n",
        "        self.data = file_path_list\n",
        "        assert mode in ['train', 'test'], f'mode needs to be either train or test, but it\\'s {mode}'\n",
        "        partition = int(len(self.data) * (1 - test_size))\n",
        "        if mode == 'train':\n",
        "            self.data = self.data[:partition]\n",
        "        else:\n",
        "            self.data = self.data[partition:]\n",
        "            self.tensor_imgs = []\n",
        "            self.labels = []\n",
        "            for i in range(len(self.data)):\n",
        "                img, label = self.__getitem__(i)                                # if we're creating the test set, we can just fetch all images\n",
        "                if len(img.shape) == 3:                                         # at once because the test set size is usually much smaller\n",
        "                    img = img.unsqueeze(0)                                      # of course this may not ALWAYS be the case...\n",
        "                self.tensor_imgs.append(img)\n",
        "                self.labels.append(label.item())\n",
        "\n",
        "            self.tensor_imgs = torch.cat(self.tensor_imgs, dim=0).type(torch.float32)\n",
        "            self.labels = torch.tensor(self.labels).type(torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        file_name = self.data[i]\n",
        "\n",
        "        if \"SNE\" in file_name or \"NEU\" in file_name or \"BNE\" in file_name:      # each file name tells us whether the image is class 0, 1, or 2\n",
        "            label = 2                                                           # so that's how we keep track of our labels\n",
        "        elif \"EO\" in file_name:\n",
        "            label = 1\n",
        "        elif \"BA\" in file_name:\n",
        "            label = 0\n",
        "\n",
        "        convert_tensor = transforms.ToTensor()                                  \n",
        "        path = self.file_path + self.idx2class[label] + \"/\" + file_name     \n",
        "        img = Image.open(path)      \n",
        "        tensor_img = convert_tensor(img)                                        # converts image to 3D torch.Tensor\n",
        "        if tensor_img.shape != (3, 363, 360):\n",
        "            tensor_img = tensor_img[:, 3:366, 3:363]                            # quick crop and reshape if the image is not uniform\n",
        "        return tensor_img.type(torch.float32), torch.tensor(label).type(torch.long)\n",
        "    \n",
        "    def get_test(self):\n",
        "        return self.tensor_imgs, self.labels"
      ],
      "metadata": {
        "id": "ZXVrOUBPYJyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the class we wrote to create 2 datasets. The creation of `testing_data` will take slightly longer because we are fetching the entire testing set into our memory. This method should take approximately 1 minute."
      ],
      "metadata": {
        "id": "wOryHlAN7_yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = BloodCell_Dataset(file_path, data, mode='train')\n",
        "testing_data = BloodCell_Dataset(file_path, data, mode='test')"
      ],
      "metadata": {
        "id": "2Fd0wsh8bia3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the training and testing set size..."
      ],
      "metadata": {
        "id": "JDpKF4ku8d0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of training_data:\\t{len(training_data)}\")\n",
        "print(f\"Size of testing_data:\\t{len(testing_data)}\")"
      ],
      "metadata": {
        "id": "90eV1Kz4eZAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the dimension before we start building our model."
      ],
      "metadata": {
        "id": "sjNoBFMB8g5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test, test_label = testing_data.get_test()\n",
        "print(test.shape)\n",
        "print(test_label.shape)"
      ],
      "metadata": {
        "id": "I5RVvyaOkVer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful! Now, our images are all of size `(3, 363, 360)`. As you have probably guessed, our images have 3 channels (RGB channels) with height of 363 pixels and width of 360 pixels (homework 5's FashionMNIST images had 28 by 28 pixels). Also, PyTorch prefers the number of channels in the first dimension (second dimension if we include batch size, such as `(N, 3, 363, 360)`)."
      ],
      "metadata": {
        "id": "ONCWWy6KcwPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly visualize our data!"
      ],
      "metadata": {
        "id": "wtXyJBPKd0XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_cell(img, label, memo=\"\"):\n",
        "    \"\"\"\n",
        "    img: a torch.Tensor or a np.ndarray\n",
        "    label: an integer\n",
        "    \"\"\"\n",
        "    if type(img) == torch.Tensor:\n",
        "        img = img.detach().cpu().numpy().squeeze()\n",
        "    img = np.transpose(img.squeeze(), (1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.title((idx2class[label]) + memo)\n",
        "    plt.show()\n",
        "\n",
        "for i in range(5):\n",
        "    img, label = training_data[i]\n",
        "    graph_cell(img, label.item())"
      ],
      "metadata": {
        "id": "x0s26fvZdsae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "In this section, we will begin creating our neural network architectures."
      ],
      "metadata": {
        "id": "WLhf1kPEgvIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's gain access to Colab's free GPU. Note, it's limited GPU, so I recommend first using CPU to make sure your model can run without errors. Once all runtime errors have been eliminated, you can turn on GPU to actually train the model. Personally, I like to turn off GPU and let my model run for ONE epoch. That way, I can make sure both my model and my training loops are correct. Another reason as to why you should debug with CPU is because GPU error messages tend to be difficult to interpret, while CPU error messages can point you to exactly where your bug is coming from.\n",
        "\n",
        "That being said, you can go to Edit > Notebook Settings > and choose \"None\" or \"GPU\" under the dropdown menu for Hardware Accelerator."
      ],
      "metadata": {
        "id": "P-4z68EOg9HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "VA2Gew3PeYJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, you will design your own network architecture.\n",
        "\n",
        "When designing a CNN, it's very good practice to keep track of the dimensions after every layer. During the debug stage, I suggest printing out the output shape while you're coding up your `forward()` and `__init__()` function."
      ],
      "metadata": {
        "id": "wz0sddIkv12l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BloodCell_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # some examples (not an actual architecture - just an example)\n",
        "        # self.conv = nn.Conv2d(3, 64, 9)\n",
        "        # self.bn = nn.BatchNorm2d(64)\n",
        "        # self.max_pool = nn.MaxPool2d(5)\n",
        "\n",
        "        \"\"\"\n",
        "        Define your layers here.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Design the process of your network.\n",
        "        \"\"\"\n",
        "\n",
        "        if len(X.shape) == 3:            # if one single image is passed, make sure it's of dimension (1, 3, H, W)\n",
        "            X = X.unsqueeze(0)\n",
        "        \n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        logits = ...\n",
        "        return logits   # do not apply softmax\n",
        "\n",
        "    def classify(self, X):\n",
        "        \n",
        "        \"\"\"\n",
        "        Write a function that outputs the labels.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "AQ9rkdXUh2Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to Homework 5's q2.ipynb and extract certain code from your previous training loop. Do not directly copy and paste the old training loop as there are some things that need to be changed here:\n",
        "\n",
        "*   You need to send your data to the device we are running the model on like so `x = x.to(device)` where device is what we specified earlier, either CUDA (GPU) or CPU. The things you need to send to the device include the data and the model.\n",
        "*   When you want to transform your tensor to NumPy, you need to make sure that your tensor is no longer part of the \"gradient descent\" mechanism and back on the CPU. You can do this by writing `x = x.detach().cpu().numpy()` (yes, it's a bit verbose, but someone reading your code can then very easily understand where the location of your tensors are).\n",
        "*   Please remove the section where you run your model on the entire training data `X_train` to collect the training accuracy. This is to save memory and speed up the training time. Please save the section where you calculate the validation accuracy though.\n",
        "*   Implement an early stopping or fall-back technique. Once you realize your model is starting to overfit or plateau, set the model to the iteration when it had the highest validation accuracy. Python's `copy` package is of great use here. In other words, every time a new accuracy is reached, we save that iteration's model weights. At the end of the function, we reset the model to that set of \"best weights.\"\n"
      ],
      "metadata": {
        "id": "KKNLNXn8kpiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from copy import deepcopy\n",
        "\n",
        "def train(model, training_data, testing_data, epochs=15, batch_size=16, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Q:  write the training loop following the schema shown above.\n",
        "\n",
        "    Inputs\n",
        "    - model: the model to be trained - a PyTorch nn.Module class object\n",
        "    - X_train, y_train, X_val, y_val: training and validation data\n",
        "    - epochs: num epochs, or the number of times we want to run through the entire training data\n",
        "    - batch_size: number of data points per batch\n",
        "    - lr: learning rate\n",
        "    - optimizer: optimizer used\n",
        "\n",
        "    Outputs\n",
        "    - losses: a list of losses\n",
        "    - accuracies: a list of accuracies\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    torch.cuda.empty_cache()\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)          # it's already shuffled\n",
        "\n",
        "    best_acc = -1\n",
        "    best_model = None\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for imgs, labels in iter(train_dataloader):         # check what imgs, labels look like\n",
        "            ...\n",
        "        \n",
        "        # calculate the validation accuracy and append the loss of this epoch\n",
        "        ...\n",
        "\n",
        "        if accuracy > best_acc:     # implement the fall-back technique mentioned in the description\n",
        "            ...\n",
        "\n",
        "        # print epoch, loss, and current test accuracy (don't delete this line - it's slightly more organized now)\n",
        "        print(f\"Epoch {epoch + 1}:\\tloss {np.round(loss.detach().cpu().numpy().item(), 4)}\\t& accuracy {np.round(accuracy, 4)}\")\n",
        "    print(f\"Resetting model... Best validation accuracy:\\t{np.round(best_acc, 4)}\")\n",
        "    model.load_state_dict(best_model.state_dict())\n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "DmifGhFcizlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's actually train our model now! If one epoch takes too long, feel free to just validate your model's `forward()` method and rerun the notebook with GPU on. Good luck!\n",
        "\n",
        "For benchmarking, my own model reached 0.80 validation accuracy with the hyperparameters provided. A successful implementation should reach at least a 0.65 accuracy - this is the accuracy the autograder will check."
      ],
      "metadata": {
        "id": "c0eSTCYzm3ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BloodCell_CNN()\n",
        "losses, accuracies = train(model, training_data, testing_data, batch_size=16, epochs=5)"
      ],
      "metadata": {
        "id": "jUKzgPNBmhWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot our predictions! If your model learned well, it should correctly classify all images besides the first one or even better. As someone who is not trained in reading blood cell slides, I have included the prediction of the model and the actual label of the slide in these plots."
      ],
      "metadata": {
        "id": "mv51TRGo4pyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "imgs = testing_data.tensor_imgs[:5]\n",
        "labels = testing_data.labels[:5]\n",
        "preds = model.classify(imgs.to(device))\n",
        "for i in range(5):\n",
        "    graph_cell(imgs[i], preds[i].item(), memo=f\"  (target: {idx2class[labels[i].numpy().item()]})\")"
      ],
      "metadata": {
        "id": "OQRfcLaumqk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Predictions\n",
        "\n",
        "Once you're done with your model training, edit the `test_path` in the following cell and it should generate a list of predictions that you will submit to Gradescope."
      ],
      "metadata": {
        "id": "XSYzNNCZvl5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empty some RAM\n",
        "torch.cuda.empty_cache()\n",
        "del testing_data\n",
        "del training_data\n",
        "\n",
        "test_path = \"/YOUR/PATH/TO/test_data.npy\"\n",
        "\n",
        "test = np.load(test_path)\n",
        "test = torch.tensor(test).type(torch.float32).to(device)\n",
        "test = test.permute(0, 3, 1, 2)\n",
        "\n",
        "# split the hidden test data in half to avoid GPU memory problems\n",
        "preds1 = model.classify(test[:71])\n",
        "preds1 = preds1.detach().cpu().numpy()\n",
        "torch.cuda.empty_cache()\n",
        "preds2 = model.classify(test[71:])\n",
        "preds2 = preds2.detach().cpu().numpy()\n",
        "preds = np.concatenate([preds1.flatten(), preds2.flatten()])\n",
        "np.save(\"/YOUR/PATH/TO/predictions.npy\", preds)\n",
        "print(\"Predictions saved!\")"
      ],
      "metadata": {
        "id": "APTAzadmtwzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short questions:\n",
        "\n",
        "\n",
        "1.   What special features did you add to your network? What worked and didn't work?\n",
        "2.   Describe how we combatted the problem of feeding too much data into the memory and causing a CUDA out of available memory error.\n",
        "\n",
        "Your answers:\n",
        "\n",
        "1.   \n",
        "2.   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HA8X8YM3AavJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "Please submit this file (make sure it has the name `q2.ipynb`), `predictions.npy`, and your model. PLEASE DO NOT submit ANY of the data. I repeat, do not submit any of the data/images used in this project or your autograding process will take forever."
      ],
      "metadata": {
        "id": "OICFGX900CUm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05papHktBCAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}