{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FWVlmdEtAFg"
   },
   "source": [
    "# Assignment 7: t-SNE for Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ9bmxxfC0j1"
   },
   "source": [
    "## 1. PCA vs t-SNE\n",
    "\n",
    "Previously we looked at PCA as a method for dimensionality reduction by transforming data using a basis of the direction of maximum variation. Here we'll compare the two methods using the out of the box methods from scikit-learn. Make sure to fill in all TODOs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MFoh_6BU29bH"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5Jcf4dN1yny"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfbf6mFi2e2t"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ej11C7an2_bx"
   },
   "source": [
    "Some Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lx9ovFYfsvIu"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.utils import shuffle\n",
    "#we are using the MNIST dataset \n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdoukUqS8c27"
   },
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWAMlp-x8iFC"
   },
   "outputs": [],
   "source": [
    "# Scale X to be between 0 and 1 (avoid magic numbers - define global constants and use them!)\n",
    "\n",
    "# TODO: Scale X to be between 0 and 1 (subtract minimum, divide by distance between minimum and maxium)\n",
    "X = ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6lhko3b14wv"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPX_XrEH4uxf"
   },
   "outputs": [],
   "source": [
    "#Dataset consists of 1797 images, each of which has 64 pixels\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yegKd-g42hWi"
   },
   "outputs": [],
   "source": [
    "# Look at the data we're working with\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(y[i])    \n",
    "    #TODO: Plot X[i] \n",
    "    #Hint: Use plt.imshow, reshape the image to be 8x8\n",
    "    #You can set cmap to 'Greys' to have the image be in grayscale\n",
    "    ...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7N0gIxo4rPm"
   },
   "source": [
    "As you can see from the above code, our input data has 64 features, each resembling a pixel in each image. We want to reduce the dimensionality of our input using both PCA and t-SNE to visualize all the data points on one chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AB5qbJbVC42y"
   },
   "source": [
    "### 1.a) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DTpzL6n4WAw"
   },
   "outputs": [],
   "source": [
    "# Carry out PCA on X\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# TODO: Run PCA on X to get the first 2 principal components\n",
    "X_pca = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0sNPCxX6hwg"
   },
   "outputs": [],
   "source": [
    "# Visualize X_pca\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    x=X_pca[:,0], y=X_pca[:,1],\n",
    "    hue=y,\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    legend=\"full\",\n",
    "    alpha=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShvSoMEPDCTW"
   },
   "source": [
    "### 1.b) t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqSKJPV97Er7"
   },
   "outputs": [],
   "source": [
    "# Carry out t-SNE on X\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "# TODO: Run t-SNE on X\n",
    "X_tsne = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2A7guY177WC"
   },
   "outputs": [],
   "source": [
    "# Visualize X_tsne\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    x=X_tsne[:,0], y=X_tsne[:,1],\n",
    "    hue=y,\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    legend=\"full\",\n",
    "    alpha=0.75\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j06S0gPS92MF"
   },
   "source": [
    "## 2. Implementing t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZEk6cgwBn4f"
   },
   "source": [
    "In this exercise, we will follow the implementation of t-SNE directly from the 2008 paper by Maaten and Hinton. It builds upon SNE (Stochastic Neighbor Embedding) and \"reduces the tendency to crowd points together in the center\n",
    "of the map.\" You can find the paper on bcourses - read through the paper before you start (sections 2 and 3.3 are particularly relevant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuaXkMCYBn4f"
   },
   "source": [
    "#### Intro/Definitions\n",
    "\n",
    "The goal of t-SNE is to define a location $y_i$ in low-dimension space for high-dimensional points $x_i$. Each point $x_i$ has a probability distribution associated with it of picking another point $x_j$ as its neighbor, defined as $p_{j|i}$ in equation 1 of the paper. $P_i$ is defined as the probability distribution (of choosing that datapoint as the neighbor of $x_i$) across all other high-dimensional points given $x_i$. Note that  Analagously for the low-dimensional points (which we have yet to find), $Q_i$ is defined as the distribution of other low-dimensional points given $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XqkcJb4Bn4g"
   },
   "source": [
    "#### Cost function description\n",
    "\n",
    "In SNE, the cost function that we're trying to minimize is the \"difference\" between $P_i$ and $Q_i$ (we want them to be similar). How do we measure difference for probability distributions? The Kullback-Leibler divergence is a measure of dissimilarity between two distributions $P$ and $Q$, defined as $$ \\sum_i \\left( P(i) \\cdot log \\frac{P(i)}{Q(i)} \\right) $$ for all values $i$ that $P$ and $Q$ take on. You can see how if $P = Q$ for all $i$ (same distribution), the log term will always be $\\log 1 = 0$, and so the KLD will be 0 (no dissimilarity). \n",
    "\n",
    "As they state in the paper, \"In particular, there\n",
    "is a large cost for using widely separated map points to represent nearby datapoints (i.e., for using a small $q_{j|i}$ to model a large $p_{j|i}$), but there is only a small cost for using nearby map points to\n",
    "represent widely separated datapoints.\" You can see that is true - if for some pair of points the $q$ is big and the $p$ is not, the term $$p\\cdot log \\frac{p}{q}$$ will be small! That will mean that it can mistranslate to low dimension sneakily without seeming like the cost is going up. That is one of the motivations behind t-SNE over SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDWo1SM68GxL"
   },
   "outputs": [],
   "source": [
    "#NOTE: for all the TODOs in this exercise, use numpy functions\n",
    "#do NOT use for loops - they will be way too slow to run tSNE\n",
    "\n",
    "def find_H_and_P_i(D=np.array([]), beta=1.0):\n",
    "    \"\"\"\n",
    "    Compute the entropy (directly connected to the perplexity) and the P-row for a specific \n",
    "    datapoint x_i, given squared euclidean distances between x_i and every other point,\n",
    "    as well as beta (the precision of the Gaussian distribution around X_i).\n",
    "    \n",
    "    As we see in the paper, the perplexity can be interpreted as a smooth measure of the \n",
    "    effective number of neighbors (non-integer). The entropy H is connected \n",
    "    to the perplexity through Perplexity =2**H. \n",
    "    \n",
    "    Inputs:\n",
    "    - D is a vector representing the squared distance of each point x_j to x_i\n",
    "    - beta is 1/(2*sigma^2) - a measure of the precision of the Gaussian distribution around X_i\n",
    "    \n",
    "    Outputs:\n",
    "    - H is the entropy (which we will use as a measure of the perplexity)\n",
    "    - P_i (a vector) is the probability density representing the similarity of x_i and each neighbor x_j \n",
    "    \"\"\"\n",
    "    # TASK: Follow equation 1 and compute the numerators of all the p_j|i.\n",
    "    \n",
    "    # TODO: Assuming beta represents whatever variance division \n",
    "    # term the algorithm decides, multiply and exponentiate to get P's \n",
    "    # numerator. Check your signs!\n",
    "    # Hint: No subtraction is necessary because x_i's position is \n",
    "    # treated as 0, since we are looking at the Gaussian around x_i.\n",
    "    \n",
    "    P_i = ...  #numerator from second equation in eq 3 in paper\n",
    "        \n",
    "    # computing perplexity in a vectorized + fast manner\n",
    "    # it's most efficient to just find the entropy\n",
    "    # https://stats.stackexchange.com/a/399328/223727\n",
    "    H = np.log2(np.sum(P)) + beta * np.sum(D * P) / np.sum(P)\n",
    "    # entropy is directly related to perplexity through Perplexity = 2**H\n",
    "    \n",
    "    # TODO: Normalize P_i to apply conditioning (1 line)\n",
    "    P_i = ...\n",
    "        \n",
    "    return H, P_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaYOkb_4Bn4i"
   },
   "source": [
    "Now we will use the perplexity (entropy) and conditional probability distributions we have computed, and brute force search for conditional Gaussians that have the same perplexity! (i.e. the $P_i$'s, or the rows of the $n$ by $n$ $P$ matrix)\n",
    "\n",
    "A way to intuitively think about this is that we should expect any two points to estimate that the \"soft\" number of neighbors is the same, if they're in the same neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEfk-qkI9xCn"
   },
   "outputs": [],
   "source": [
    "def find_P_from_X(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "    Performs a binary search to find the values of beta\n",
    "    such that each conditional Gaussian has the same desired perplexity.\n",
    "    \n",
    "    Computes the values of P given these values of beta \n",
    "    and returns P.\n",
    "    \n",
    "    Inputs: \n",
    "    - X - an n by d matrix of the data, where n is the number of samples\n",
    "        and d is the number of features\n",
    "    - tol - the tolerance for H. Stop if H is within tol of the log of\n",
    "        the desired perplexity\n",
    "    - perplexity - the desired perplexity. Intuitively, a guess about the\n",
    "        number of close neighbors each datapoint has\n",
    "        \n",
    "    return:\n",
    "    - P - a matrix where each row is the probability density that\n",
    "        represents how likely x_i is to choose each other datapoint x_j\n",
    "        as its neighbor    \n",
    "    \"\"\"\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    #D_ij is the squared distance from x_i to x_j\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "\n",
    "    # NOTE: the P you compute should be of this shape. \n",
    "    # Make sure you understand why!\n",
    "    P = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    \n",
    "    # NOTE: the log of the perplexity is our target entropy\n",
    "    logU = np.log2(perplexity)\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P for point %d of %d...\" % (i, n))\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        # np.r_ just efficiently concatenates along the row\n",
    "        # https://stackoverflow.com/a/30597960/10302846\n",
    "        D_i = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
    "        \n",
    "        # TODO: Utilize your newly written function to get perplexity and Pvals \n",
    "        # (1 line)\n",
    "        H, P_i = ...\n",
    "\n",
    "        # TODO: Evaluate whether the perplexity is within tolerance - use logU (1 line)\n",
    "        H_diff = ... \n",
    "        # Hint - don't apply absolute value yet - we need to know the sign\n",
    "        # of this value for binary search! You can apply abs in the loop cond.\n",
    "\n",
    "        tries = 0\n",
    "        \n",
    "        # TODO:\n",
    "        # Write a loop condition that continues if we're both below the max\n",
    "        # number of tries (50), and the absolute difference is greater than \n",
    "        # the tolerance.\n",
    "\n",
    "        while ... :\n",
    "\n",
    "            # TASK: If not, increase or decrease the precision (beta)\n",
    "            # What follows here is a version of the standard binary search checks.\n",
    "            # The idea is that if we're too high (H_diff positive), we can set the \n",
    "            # new min to be our current beta, and move our beta towards the max.\n",
    "            # If the max is currently infinity, we should just double beta.\n",
    "            # If the max is an actual number, set beta to be the avg of it and max.\n",
    "\n",
    "            if H_diff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    # adjust the beta\n",
    "                    # TODO: \n",
    "                    beta[i] = ...\n",
    "                else:\n",
    "                    # adjust the beta\n",
    "                    # TODO: \n",
    "                    beta[i] = ...\n",
    "            else:\n",
    "                # Opposite of above (if H_diff is negative)\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    # TODO: \n",
    "                    beta[i] = ...\n",
    "                else:\n",
    "                    # TODO: \n",
    "                    beta[i] = ...\n",
    "\n",
    "            # TODO: Recompute the values again using your function and the new betas\n",
    "            H, thisP = ...\n",
    "            \n",
    "            # TODO: Recompute H_diff\n",
    "            # (same as \"Evaluate whether the perplexity is within tolerance\")\n",
    "            H_diff = ...\n",
    "            \n",
    "            tries += 1\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = P_i\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(0.5 / beta)))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONrq9ewPBn4l"
   },
   "outputs": [],
   "source": [
    "def pca(X=np.array([]), n_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        n_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
    "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
    "    Y = np.dot(X, M[:, 0:n_dims])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqQ2zUuaBn4o"
   },
   "source": [
    "Now, we will use the Student t-distribution with one degree of freedom to compute $Q$, the matrix of joint probabiltiies in low dimensions (see equation 4 of the paper).\n",
    "\n",
    "When calculating the numerator of $Q$ note that $ \\lVert y_i - y_j \\rVert^2 $ can be expanded as $y_i^{T} y_i - 2y_i^T y_j + y_j^T y_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVYpbhB_Bn4o"
   },
   "outputs": [],
   "source": [
    "def tsne(X=np.array([]), n_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to n_dims dimensions. \n",
    "        \n",
    "        Uses PCA to reduce X to initial_dims dimensions, first.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(n_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(n_dims) != n_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims).real\n",
    "    (n, d) = X.shape\n",
    "    max_iter = 400\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 500\n",
    "    min_gain = 0.01\n",
    "    Y = np.random.randn(n, n_dims)\n",
    "    dY = np.zeros((n, n_dims))\n",
    "    iY = np.zeros((n, n_dims))\n",
    "    gains = np.ones((n, n_dims))\n",
    "    tolerance = 1e-5\n",
    "    \n",
    "    #TODO:\n",
    "    # Compute P-values using your previous function\n",
    "    # remember to pass tolerance and perplexity to the function\n",
    "    P = ...\n",
    "    \n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    P = P * 4 # early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # TASK: Compute pairwise affinities (Q)\n",
    "        \n",
    "        # SUBTASK: compute the numerator `num`\n",
    "        \n",
    "        \n",
    "        #TODO: first, find the norm squared \n",
    "        \n",
    "        #Hints:\n",
    "        #look at the expansion of norm squared in the above cell\n",
    "        #To find the matrix of norm squared, you can \n",
    "        #  find a matrix for each term and sum them together\n",
    "        #For the first and third terms, you will need to use np.square and np.sum\n",
    "        #  consider the orientation of Y - is y_i a row or a column?\n",
    "        #for the middle (-2) term, you will need to compute the outer product \n",
    "        #  of Y with itself (use np.dot)\n",
    "        #remember - do NOT use for loops - they will be too slow\n",
    "        \n",
    "        norm_sq = ... #this is a matrix where norm_sq[i][j] = ||y_i - y_j||^2\n",
    "        \n",
    "        # ^ last hint: we already had to find squared distances elsewhere in the code, for ||x_i - x_j||^2\n",
    "        \n",
    "        #TODO: next, find the numerator using norm_sq - look at equation 4 in pdf\n",
    "        num = ...\n",
    "        \n",
    "        # END OF SUBTASK\n",
    "        \n",
    "        # TODO: set the diagonal of numerator to 0 (because q_ii = 0), and normalize it to get Q\n",
    "        # you can index into matrix using [range(a), range(a)] to get the diagonal elements\n",
    "        # (2 lines)\n",
    "        ...\n",
    "        Q = ...\n",
    "        \n",
    "        # END OF TASK\n",
    "        \n",
    "        # this makes Q nonzero/non-negative, because floats are terrible\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (n_dims, 1)).T * (Y[i, :] - Y), 0)\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
    "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iY = momentum * iY - eta * (gains * dY)\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            # TODO: Implement KL Divergence cost function on P and Q\n",
    "\n",
    "            C = ...\n",
    "            # HINT: make sure you wrap it with an np.sum so it's a scalar\n",
    "\n",
    "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
    "\n",
    "        # Stop exaggerating about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4.\n",
    "\n",
    "    # Return solution\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHwu_wO5Bn4r"
   },
   "outputs": [],
   "source": [
    "# Run TSNE again to check that the implementation's output matches the library version's output!\n",
    "X_tsne = tsne(X)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    x=X_tsne[:,0], y=X_tsne[:,1],\n",
    "    hue=y,\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    legend=\"full\",\n",
    "    alpha=0.75\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xeu3d7s7NdwM"
   },
   "source": [
    "If your implementation is correct, you should see good separation much like the run in Question 1, though the shapes and locations of the clusters likely won't be the same (due to our implementation and parameters differing slightly from the sklearn t-SNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwLZ70x3KB6n"
   },
   "source": [
    "## 3. Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Fh5QZ8_KIjG"
   },
   "source": [
    "### 3.a) To optimize the performance of the method, we scaled P by 4 for the first 100 iterations of gradient descent. How does this improve performance?\n",
    "\n",
    "see: Section 3.4 in the paper\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7zVk9oGVbtD"
   },
   "source": [
    "### 3.b) In the above implementation, we used PCA to first reduce the input to 50 dimensions/features - why is it preferred to run t-SNE on data that doesn't have a high number of dimensions?\n",
    "\n",
    "see: Section 6.2, number 2 in the paper\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDexfTq7O8QK"
   },
   "source": [
    "### 3.c) t-SNE is known to be a 'nonlinear' dimensionality reduction method. As such the distance between points does not accurately reflect distance in the original space. Based on the math behind the technique, explain why t-SNE is nonlinear.\n",
    "\n",
    "see: Section 2 in the paper\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeCij84RYJTH"
   },
   "source": [
    "### 3.d) Explain the improvement of this method over SNE - specifically, why is the distance between two points in the lower-dimensional space defined as (1) rather than the definition of distance used for the higher dimensional space (2), ignoring the symmetrization?\n",
    "\n",
    "\n",
    "$$q_{i j} = \\frac{(1+||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1+||y_k - y_l||^2)^{-1}} \\;\\;\\;\\;\\;\\;  (1)$$\n",
    "\n",
    "$$p_{i j} = \\frac{\\text{exp}(-||x_i - x_j||^2 / 2\\sigma^2)}{\\sum_{k \\neq l}\\text{exp}(-||x_k - x_l||^2/ 2\\sigma^2)} \\;\\;\\;\\;\\;\\;  (2)$$\n",
    "\n",
    "see: Section 3 in the paper\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-R2DUfwdNR4"
   },
   "source": [
    "Save this notebook as a pdf and submit to gradescope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
