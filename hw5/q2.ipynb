{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 5 Q2\n",
        "\n",
        "Up to this point, we have created our own models and practiced implementing existing machine learning techniques and derivations. It is now time to move on to more recent advancements of machine learningâ€“Deep Learning. In this homework, I will demonstrate how PyTorch can be used and some common and useful functionalities. In the end, you will be required to create your own model that will be trained on a dataset."
      ],
      "metadata": {
        "id": "grF7sAQBffrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Tensors\n",
        "\n",
        "We have made you practice NumPy extensively over the past few weeks: to understand the functions, the dimensions, and the axes. In PyTorch, the package doesn't like to work with NumPy objects. Instead, it works with its own version of NumPy called Torch.Tensors. Torch.Tensor is extremely similar to NumPy and we will show it here.\n",
        "\n",
        "Let's make a random matrix to play with."
      ],
      "metadata": {
        "id": "7MxDWa8ZgJjh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAurHyHnfdk6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "X = torch.rand(5, 8)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily create the NumPy object of the same tensor using the `.numpy()` method.\n",
        "\n"
      ],
      "metadata": {
        "id": "5_fl1IKxiAk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_np = X.numpy()    # create a NumPy version of X\n",
        "print(X_np)\n",
        "print(type(X_np))"
      ],
      "metadata": {
        "id": "HI0TkbDMhgWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And change it back to Torch.Tensor by wrapping it around with torch.tensor()."
      ],
      "metadata": {
        "id": "ALcdk3BAiU3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_np)\n",
        "print(X)\n",
        "print(\"Shape:\\t\", X.shape)\n",
        "print(\"X.shape is still represented by a tuple:\\t\", X.shape == (5, 8))\n",
        "print(\"Type:\\t\", type(X))"
      ],
      "metadata": {
        "id": "RX3ob7vRgu6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing works the same as NumPy as well."
      ],
      "metadata": {
        "id": "HHzGObF8hMYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[1:3, 2:5])      # certain section of the matrix\n",
        "print(X[[0, 2, 4], :])  # row 0, 2, and 4"
      ],
      "metadata": {
        "id": "0Ika9wZIg7Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boolean slicing works the same as NumPy as well."
      ],
      "metadata": {
        "id": "cgvrUbLKiuF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bool_idx = X < 0.5\n",
        "print(bool_idx)\n",
        "X_bool_filter = torch.clone(X)  # torch.clone() can help us deep copy a matrix so we don't accidentally modify the original copy\n",
        "X_bool_filter[bool_idx] = 99    # using the boolean slicing, we change the values less than 0.5 to 99\n",
        "print(torch.round(X_bool_filter, decimals=4))\n",
        "print(X)                        # notice how the original tensor is unchanged, that's why torch.clone() is important"
      ],
      "metadata": {
        "id": "R3t9qJexixaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we have our favorite mean, var, and std functions.\n",
        "Now, the only difference between tensors and NumPy is that tensors use the keyword **dim** in place of NumPy's **axis**, like so."
      ],
      "metadata": {
        "id": "s2njEG_3hhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.mean(X))\n",
        "print(torch.var(X))\n",
        "print(torch.std(X))"
      ],
      "metadata": {
        "id": "qfvKMNDfheLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.mean(X, dim=0))\n",
        "print(torch.var(X, dim=0))\n",
        "print(torch.std(X, dim=0))"
      ],
      "metadata": {
        "id": "ZA3m8JfRigLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some linear algebra operations:\n"
      ],
      "metadata": {
        "id": "NFSSgDexlFHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.rand(8, 3)\n",
        "# dot product - two ways of doing so (NumPy also takes @ for dot products)\n",
        "print(X @ Z)\n",
        "print(torch.matmul(X, Z))\n",
        "print(torch.einsum(\"ij,jk->ik\", [X, Z]))    # this is torch.einsum - not important for this class but a cool function regardless\n",
        "print((X @ Z).shape)"
      ],
      "metadata": {
        "id": "ACdPdLRtikn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate eigenvectors and eigenvalues with PyTorch!"
      ],
      "metadata": {
        "id": "-KMI-BxVmabm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(4, 4)\n",
        "print(torch.linalg.eig(X))"
      ],
      "metadata": {
        "id": "7Sgv_i1blZVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network 101\n",
        "\n",
        "Now, we will learn how to create our own neural networks using PyTorch. Our model is still pretty light since this is our first exposure to PyTorch. Next week, we will be learning how to train on models on GPU's to speed up training time!"
      ],
      "metadata": {
        "id": "uKWZ-8ximfvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we will mount on Google Drive, so you can access your files from Google Drive like how you would access files from your local computer."
      ],
      "metadata": {
        "id": "MaORQBHJn3Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WTegbN2ZoXOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bd1503-96d6-48bd-bccf-9909a57ecd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some standard imports!"
      ],
      "metadata": {
        "id": "EaKrU7HzpUi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "m9P3-Ro-mHvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be working with a very popular deep learning dataset Fashion MNIST. Each data point is a 28x28 image of some type of clothes."
      ],
      "metadata": {
        "id": "UDrlQoGMrTIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/bioeng245_sp2023/hw5/\"      # ENTER YOUR OWN FILE PATH\n",
        "data = pd.read_csv(file_path + \"fashion_mnist_data.csv\", sep=',')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kghvXO08nqjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)   # (N, D + 1) (one column is the label column)"
      ],
      "metadata": {
        "id": "2MzrHvzirEsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, transform the DataFrame data to 2 NumPy arrays X and y, where X is the data matrix with 784 pixels per row. y should be a NumPy vector of the labels. From the cell above, we see that there are 8,000 samples to work with. Each sample has 784 features, or pixels, where the first column, \"label\", is the label of that sample. The final shape of X should be `(8000, 784)` and y should be `(8000, )`."
      ],
      "metadata": {
        "id": "Hv49eFRJqvZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "...\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "cg824aI6qehg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly visualize some samples. You should see some images of clothes and the correct labels if you implemented the above cell correctly."
      ],
      "metadata": {
        "id": "L8pXuVsesIks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = [\"T-Shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Boot\"]\n",
        "\n",
        "def show_image_labels(img, label):\n",
        "    plt.imshow(img.reshape(28, 28))\n",
        "    plt.title(labels[label])\n",
        "    plt.show()\n",
        "\n",
        "for i in range(5):\n",
        "    show_image_labels(X[i], y[i])"
      ],
      "metadata": {
        "id": "kUakZ4dyq_yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will build our first neural network!"
      ],
      "metadata": {
        "id": "dTxzqTEYtkYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sample_Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        input_dim =  ...    # what should be the input dimension? what are we feeding the network?\n",
        "        output_dim = ...    # what should be the output dimension? what do we want from the network after running model(), or also known as model.forward()?\n",
        "        self.input_layer = nn.Linear(input_dim, 126)\n",
        "        self.hidden_layer = nn.Linear(126, 256)\n",
        "        self.output_layer = nn.Linear(256, output_dim)\n",
        "\n",
        "    # self.forward is the main function that you will run your model with!\n",
        "    # model(X) is equivalent as model.forward(X).\n",
        "    # you will see how it's used in just a bit!\n",
        "    # nothing to implement here - an example of how you the forward method works\n",
        "    # as you can see, X is first passed into the input layer, then a non-linearity ReLU\n",
        "    # then, the hidden layer followed by a non-linearity ReLU\n",
        "    # in the end, the output_layer produces a logit matrix based on how many classes\n",
        "    # we want it to classify\n",
        "    def forward(self, X):\n",
        "        X = self.input_layer(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.hidden_layer(X)\n",
        "        X = F.relu(X)\n",
        "        logits = self.output_layer(X)\n",
        "        return logits\n",
        "\n",
        "    # we will create our own function to help with prediction\n",
        "    # we will utilize F.softmax and torch.argmax!\n",
        "    def classify(self, X):\n",
        "        \"\"\"\n",
        "        Q:  create a function classify that will take in X data points and produce\n",
        "            the predicted classification of these points.\n",
        "\n",
        "        HINT: use torch.softmax() and torch.argmax()! you NEED them.\n",
        "\n",
        "        Inputs\n",
        "        - X: the torch.tensor matrix to be classified with shape (N, D)\n",
        "\n",
        "        Outputs\n",
        "        - labels: a torch.tensor with the shape (N, ), each item being X[i]'s \n",
        "                  classification prediction\n",
        "        \"\"\"\n",
        "        X = torch.tensor(X).type(torch.float32)     # enforce smooth-running with the model\n",
        "        logits = self(X)\n",
        "        ...\n",
        "        return labels.type(torch.long)"
      ],
      "metadata": {
        "id": "c-8aNVErtHXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write code to transform X and y into Tensors. `X` needs to be of type `torch.float32` and `y` needs to be of type `torch.long`. Google how you can cast a tensor to some certain type with PyTorch."
      ],
      "metadata": {
        "id": "epYwFybOvIPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "X, y = ..., ..."
      ],
      "metadata": {
        "id": "XFZAMEgFvH2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate our model and see what the outputs look like! It should look like (2, 10) because we fed in 2 samples and we're classifying one of the 10 possible classes!"
      ],
      "metadata": {
        "id": "z8Vrj4RHv5Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sample_Network()\n",
        "out = model(X[:2])\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "xzkJUASct-rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply softmax to the output, we get the probability distribution of each class!"
      ],
      "metadata": {
        "id": "hUl3IeKYwJTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.classify(X[:2])\n",
        "print(preds)    # the model thinks that the first 2 samples are these classes"
      ],
      "metadata": {
        "id": "CZcJ98WSwIbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see if the model is doing a good job\n",
        "show_image_labels(X[0], preds[0])       # this should be a T-shirt, not a shirt - they are two different classes\n",
        "show_image_labels(X[1], preds[1])"
      ],
      "metadata": {
        "id": "ZoRW9az6vEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically, the first image is supposed to be a t-shirt, not a shirt, soooo the model got both wrong (if it somehow did it got lucky)... Why? Well, we haven't trained it yet! And that's our next section!"
      ],
      "metadata": {
        "id": "14PEE-Wpx6j5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing the Training Loop\n",
        "\n",
        "This is arguably one of the most important components of Deep Learning and PyTorch. We need to write our own training loop that will repeatedly train our model a number of epochs of times, based on our own specifications.\n",
        "\n",
        "A typical training loop will have the following structure:\n",
        "\n",
        "\n",
        "*   Take in the model and the data X_train, y_train, X_val, and y_val.\n",
        "*   Take in hyperparameters such as learning rate, batch size, optimizer, schedulers, etc.\n",
        "*   Iterate through the epochs\n",
        "*   Each epoch, iterate through the batches and keep track of the losses and the metrics of choice (accuracy in our case)."
      ],
      "metadata": {
        "id": "1nnMk8NIyGnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train(model, X_train, y_train, X_val, y_val, epochs=15, batch_size=32, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Q:  write the training loop following the schema shown above.\n",
        "\n",
        "    Inputs\n",
        "    - model: the model to be trained - a PyTorch nn.Module class object\n",
        "    - X_train, y_train, X_val, y_val: training and validation data\n",
        "    - epochs: num epochs, or the number of times we want to run through the entire training data\n",
        "    - batch_size: number of data points per batch\n",
        "    - lr: learning rate\n",
        "    - optimizer: optimizer used\n",
        "\n",
        "    Outputs\n",
        "    - losses: a list of losses\n",
        "    - accuracies: a list of validation accuracies\n",
        "    - train_accs: a list of training accuracies\n",
        "    \"\"\"\n",
        "\n",
        "    batches = ...   # using batch_size, determine the number of batches needed\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()                             # read the write-up for an explanation on CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)     # read the write-up for an explanation on Adam\n",
        "\n",
        "    losses = []\n",
        "    train_accs = []\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(batches):\n",
        "            X_batch = ...\n",
        "            y_batch = ...\n",
        "\n",
        "            logits = ...\n",
        "\n",
        "            loss = loss_fn(logits, y_batch)\n",
        "\n",
        "            # these 3 functions will follow you whenever you train a model with PyTorch\n",
        "            optimizer.zero_grad()   # erases the gradients from the previous epoch (sets all gradients to 0)\n",
        "            loss.backward()         # calculates the gradients with respect to every single weight matrix in the model\n",
        "            optimizer.step()        # takes ONE learning step with the gradients just calculated\n",
        "\n",
        "        # feel free to use sklearn's accuracy_score function\n",
        "        # calculate the training accuracy\n",
        "        ...\n",
        "\n",
        "        # calculate the validation accuracy and append the loss of this epoch\n",
        "        ...\n",
        "\n",
        "        # print epoch, loss, and current test accuracy\n",
        "        print(f\"Epoch {epoch}:\\tloss {loss} & accuracy {accuracy}\")\n",
        "    \n",
        "    return losses, accuracies, train_accs"
      ],
      "metadata": {
        "id": "dEQ3JOj_xWj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you implemented the above code `Sample_Network` and `train()` correctly, you should see close to a 0.79-0.80 accuracy."
      ],
      "metadata": {
        "id": "Y6YVQTtqoiWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sample_Network()\n",
        "losses, accuracies, train_accs = train(model, X[:7000], y[:7000], X[7000:], y[7000:], epochs=5)"
      ],
      "metadata": {
        "id": "YXz0CsoqzFlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how much the model has learned\n",
        "preds = model.classify(X[:5])\n",
        "for i in range(5):\n",
        "    show_image_labels(X[i], preds[i])"
      ],
      "metadata": {
        "id": "PUK5BZ8d1jSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to what we had earlier, the model clearly learned *something*. It is currently able to differentiate among different images of clothes with an 80% accuracy!"
      ],
      "metadata": {
        "id": "RbHKFFHj3k4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Your Own Model\n",
        "\n",
        "Now, fill in the following class and make your own neural network model that can classify the clothes with a validation accuracy of 90%!\n",
        "\n",
        "Using the previous `Sample_Network` as an example, please implement `My_Network`, a class that is able to produce `num_layers` of hidden layers (NOT including the input and the output layers) where each hidden layer has `hidden_size` units. Additionally, please **add one or more extra features** to boost your performance. Some inspirations: adding Dropout() layers, Batchnorm() layers, normalize inputs, learning rate schedulers, etc.\n",
        "\n",
        "When you're done with this section, click on the folder icon to your left (for those using Google Colab), and you should be able to download `my_model.pt` and `predictions.npy` - the 2 files you will submit to Gradescope."
      ],
      "metadata": {
        "id": "xfvSgdCR38_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Network(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        super().__init__()\n",
        "        ...\n",
        "    \n",
        "    def forward(self, X):\n",
        "        ...\n",
        "\n",
        "    def classify(self, X):\n",
        "        ..."
      ],
      "metadata": {
        "id": "Jm0xDDKP3h4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = My_Network(...)\n",
        "losses, accuracies, train_accs = train(my_model)"
      ],
      "metadata": {
        "id": "PAc_H2sn4X3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(my_model.state_dict(), file_path + \"my_model.pt\")       # save your model - do NOT change the name of \n",
        "                                                                   # the model or else the autograder won't recognize it!"
      ],
      "metadata": {
        "id": "N63tBxAK4guN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_classifications(model, file_path):\n",
        "    test_df = pd.read_csv(file_path, sep=',')\n",
        "    X = test_df.values\n",
        "    X = torch.tensor(X).type(torch.float32)\n",
        "    out = model.classify(X)\n",
        "    np.save(\"predictions.npy\", out)\n",
        "    print(\"predictions.npy saved!\")\n",
        "\n",
        "get_test_classifications(my_model, \"/YOUR/PATH/TO/fashion_mnist_test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecOZy7xdgcfs",
        "outputId": "f59650f2-a17e-4251-d288-a70aa286c3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions.npy saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph out your training vs. validation accuracy. These values should be available after running your training loop."
      ],
      "metadata": {
        "id": "bdJEvqitkI3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graph(accuracies, training_accs):\n",
        "    \"\"\"\n",
        "    Q:  graph out the accuracies and training accuracies.\n",
        "        make sure you label which curve is the validation/training accuracy.\n",
        "        labels and titles are required.\n",
        "\n",
        "    Inputs\n",
        "    - accuracies: list of floats with length epochs\n",
        "    - training_accs: list of floats with length epochs\n",
        "\n",
        "    Outputs\n",
        "    - None\n",
        "    \"\"\"\n",
        "    ..."
      ],
      "metadata": {
        "id": "LlA4P5WKkShZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Short report\n",
        "\n",
        "\n",
        "1.   What extra feature did you end up implementing? Why? Did it improve model performance?\n",
        "2.   What did you observe after plotting the training and validation accuracy? Were there any overfitting or underfitting?\n",
        "\n",
        "\n",
        "Your response:\n",
        "\n",
        "\n",
        "\n",
        "1.   ...\n",
        "2.   ...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GSYN_7gOksTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "We have explored the very fundamentals of PyTorch in this homework, and the tools we have currently worked with is only the tip of the iceberg! In our next homework, we will do some heavy convolutional neural networks and learn how we can send our model to the GPU for a dramatically faster training time."
      ],
      "metadata": {
        "id": "m45By_XzlKjl"
      }
    }
  ]
}